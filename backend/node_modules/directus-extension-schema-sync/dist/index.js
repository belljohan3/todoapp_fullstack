import t,{readdir as e,readFile as i,writeFile as a,access as s}from"fs/promises";import o,{resolve as n}from"path";import{fileURLToPath as r,pathToFileURL as c}from"url";import{createHash as l}from"crypto";function h(t){let e=!1,i=!1;return new Proxy(t,{async apply(t,a,s){if(e)i=!0;else do{e=!0,i=!1;try{return await Reflect.apply(t,a,s)}catch(t){console.error("Error in condensed action:",t)}finally{e=!1}}while(i)}})}const d={role:"",admin:!0};function p(t,e){return import(c(n(t,e)).href)}class f{static get schemaDir(){return n(process.cwd(),"schema-sync")}static get dataDir(){return n(f.schemaDir,"data")}static get hashFile(){return n(f.schemaDir,"hash.txt")}static utcTS(t=(new Date).toISOString()){return t.replace("T"," ").replace(/\.\d*Z/,"")}static async updateExportMeta(t=""){const s=l("sha256"),o=await e(f.dataDir);for(const t of o)if(t.endsWith(".json")){const e=await i(`${f.dataDir}/${t}`,{encoding:"utf8"});s.update(e)}const n=s.digest("hex");if(n===t)return!1;const r=f.utcTS(),c=n+"@"+r;return await a(this.hashFile,c),{hash:n,ts:r}}static async fileExists(t){try{return await s(t),!0}catch{return!1}}static async getExportMeta(){if(await this.fileExists(this.hashFile)){const t=await i(this.hashFile,{encoding:"utf8"}),[e,a]=t.split("@");if(e&&a&&"Invalid Date"!==new Date(a).toString())return{hash:e,ts:a}}return null}}function u(t,e){if(t===e)return!0;if("object"!=typeof t||null===t||"object"!=typeof e||null===e)return!1;const i=Object.keys(t),a=Object.keys(e);if(i.length!==a.length)return!1;for(let s of i){if(!a.includes(s))return!1;if(!u(t[s],e[s]))return!1}return!0}function m(t,e){if(!e)return t;const i={};let a=!1;return Object.keys(t).forEach((s=>{u(t[s],e[s])||(i[s]=t[s],a=!0)})),a?i:null}function g(t){if("object"!=typeof t||null===t)return t;if(Array.isArray(t))return t.map(g);const e={};return Object.keys(t).sort().forEach((i=>{e[i]=g(t[i])})),e}const w={excludeFields:[],query:{limit:-1}};class y{constructor(t,e,i=w,a){let s;this.logger=a,this.options={excludeFields:[],query:{limit:-1},...i},this._getService=async()=>s||(s=await e(t)),this.collection=t;const o=this.options.prefix?`${this.options.prefix}_${t}`:t;this.filePath=`${f.dataDir}/${o}.json`}_getService;collection;options;filePath;get name(){return this.collection}_persistQueue=h((()=>this.exportCollectionToFile()));export=()=>this._persistQueue();async load(t=!1){if(await f.fileExists(this.filePath)){const e=await i(this.filePath,{encoding:"utf8"});return await this.loadJSON(e,t)}return null}exportCollectionToFile=async()=>{const t=await this.getJSON();this.logger.debug(`Exporting ${this.collection}`),await a(this.filePath,t)};_settings=null;async settings(){if(this._settings)return this._settings;const t=(await this._getService()).schema.collections[this.collection];if(!t)throw new Error(`Schema for ${this.collection} not found`);const e=this.options.excludeFields||[];if(e.includes(t.primary)&&!this.options.getKey)throw new Error(`Can't exclude primary field ${t.primary} without providing a getKey function`);let i=[];for(const a in t.fields){t.fields[a].alias||e.includes(a)||i.push(a)}const a=e=>e[t.primary],s=this.options.getKey||a,o=this.options.query||{};o.fields=i,o.limit=o.limit||-1,o.sort=o.sort||[t.sortField||t.primary];const n=e.includes(t.primary)?{...o,fields:[...i,t.primary]}:o;return this._settings={inclFields:i,exclFields:e,linkedFields:this.options.linkedFields||[],getKey:s,getPrimary:a,query:o,queryWithPrimary:n}}async getJSON(){const t=await this._getService(),{query:e}=await this.settings();let i=await t.readByQuery(e);if(!i.length)return"";if(this.options.onExport){const e=[];for(const a of i){const i=await this.options.onExport(a,t);i&&e.push(i)}i=e}return JSON.stringify(g(i),null,2)}async sortbyIfLinked(t){const{getPrimary:e,linkedFields:i}=await this.settings();if(!i.length)return!1;const a=t.reduce(((t,i)=>(i.__dependents=[],t[e(i)]=i,t)),{});return t.forEach((t=>{for(const e of i){const i=t[e];i&&a[i]&&a[i].__dependents.push(t)}})),t.sort(((t,e)=>this.countDependents(e)-this.countDependents(t))),t.forEach((t=>delete t.__dependents)),!0}countDependents(t){return t.__dependents.length?t.__dependents.reduce(((t,e)=>t+this.countDependents(e)),t.__dependents.length):0}async loadJSON(t,e=!1){if(!t)return null;const i=JSON.parse(t);if(!Array.isArray(i))throw new Error(`Invalid JSON: ${t}`);const a=await this._getService(),{getKey:s,getPrimary:o,queryWithPrimary:n}=await this.settings(),r=await a.readByQuery(n),c={},l=[];r.forEach((t=>{c[s(t)]?(this.logger.warn(`Will delete duplicate ${this.collection} item found #${o(t)}`),l.push(o(t))):c[s(t)]=t}));const h={},d=[],p=new Set;for(let t of i){if(this.options.onImport&&(t=await this.options.onImport(t,a),!t))continue;const e=s(t);if(p.has(e))continue;const i=c[e];if(i){delete c[e];const a=m(t,i);a&&(h[o(i)]=a)}else d.push(t);p.add(e)}if(d.length>0)if(this.logger.debug(`Inserting ${d.length} x ${this.collection} items`),await this.sortbyIfLinked(d))for(const t of d)await a.createOne(t);else await a.createMany(d);const f=Object.entries(h);if(f.length>0){this.logger.debug(`Updating ${f.length} x ${this.collection} items`);for(const[t,e]of f)await a.updateOne(t,e)}return async()=>{if(!e){const t=l.concat(Object.values(c).map(o));t.length>0&&(this.logger.debug(`Deleting ${t.length} x ${this.collection} items`),await a.deleteMany(t))}}}}class x{constructor(t){this.logger=t}exporters=[];addExporter(t){this.exporters.push(t)}addCollectionExporter(t,e){for(let i in t){const a=t[i];this.exporters.push({watch:a.watch,exporter:new y(i,e,a,this.logger)})}}async loadAll(t=!1){await this._loadNextExporter(0,t)}async _loadNextExporter(t=0,e=!1){if(!(t>=this.exporters.length))try{const i=await this.exporters[t].exporter.load(e);await this._loadNextExporter(t+1,e),i&&await i()}catch(e){throw this.logger.error(`Failed loading "${this.exporters[t].exporter.name}".`),e}}attachAllWatchers(t,e){const i=["create","update","delete"];this.exporters.forEach((({watch:a,exporter:s})=>{a.forEach((a=>{i.forEach((i=>{t(`${a}.${i}`,(async()=>{await s.export(),await e()}))}))}))}))}async exportAll(){console.log("Exporting ",this.exporters.length," exporters"),await Promise.all(this.exporters.map((t=>t.exporter.export())))}}const _={postgres:[function(t){return t.fields.map((t=>(t.schema?.default_value=="nextval('"+t.schema?.table+"_"+t.schema?.name+"_seq'::regclass)"&&(t.schema.default_value=null,t.schema.has_auto_increment=!0),t))),t}]};class S{constructor(t,e){this.logger=e,this._getSchemaService=()=>t(),this._filePath=`${f.dataDir}/schema.json`}_filePath;_getSchemaService;_exportHandler=h((()=>this.createAndSaveSnapshot()));get name(){return"schema"}export=()=>this._exportHandler();load=async()=>{const t=this._getSchemaService();if(await f.fileExists(this._filePath)){const e=await i(this._filePath,{encoding:"utf8"});if(e){const i=JSON.parse(e),a=i.snapshot,s=i.hash,o=await t.snapshot(),n=t.getHashedSnapshot(o).hash;if(n===s)return void this.logger.debug("Schema is already up-to-date");const r=await t.diff(a,{currentSnapshot:o,force:!0});null!==r&&await t.apply({diff:r,hash:n})}}};createAndSaveSnapshot=async()=>{const t=this._getSchemaService();let e=await t.snapshot();e=function(t){return _[t.vendor]?.length?_[t.vendor].reduce(((t,e)=>e(t)),t):t}(e);let i=t.getHashedSnapshot(e).hash,s=JSON.stringify({snapshot:e,hash:i},null,2);await a(this._filePath,s)}}class v{db;tableName="directus_settings";rowId=1;_locking=!1;_locked=!1;constructor(t){this.db=t}async lockForUpdates(t,e){if(this._locked||this._locking)return!1;this._locking=!0;const i=await this.db.transaction((async i=>!!(await i(this.tableName).select("*").where("id",this.rowId).where("mv_locked",!1).andWhereNot("mv_hash",t).andWhere("mv_ts","<",e).orWhereNull("mv_ts").forUpdate()).length&&(await i(this.tableName).where("id",this.rowId).update({mv_locked:!0}),this._locked={hash:t,ts:e},!0)));return this._locking=!1,i}async commitUpdates(){return!!this._locked&&(await this.db(this.tableName).where("id",this.rowId).update({mv_hash:this._locked.hash,mv_ts:this._locked.ts,mv_locked:!1}),this._locked=!1,!0)}async forceCommitUpdates(t,e){return await this.db(this.tableName).where("id",this.rowId).update({mv_hash:t,mv_ts:e,mv_locked:!1}),this._locked=!1,!0}async releaseLock(){return!!this._locked&&(await this.db(this.tableName).where("id",this.rowId).update({mv_locked:!1}),this._locked=!1,!0)}async ensureInstalled(){const t="directus_settings";await this.db.schema.hasColumn(t,"mv_hash")||await this.db.schema.table(t,(t=>{t.string("mv_hash").defaultTo("").notNullable(),t.timestamp("mv_ts",{useTz:!0}).defaultTo("2020-01-01").notNullable(),t.boolean("mv_locked").defaultTo(!1).notNullable()}))}}const E=async({action:e,init:i},{env:a,services:s,database:n,getSchema:c,logger:l})=>{const{SchemaService:u,ItemsService:m}=s;let g;const w=async()=>g||(g=await c({accountability:d,database:n})),y=()=>new u({knex:n,accountability:d}),_=async t=>new m(t,{schema:await w(),accountability:d,knex:n}),E=new v(n);let b;const k=async()=>{if(!b){b=new x(l),b.addExporter({watch:["collections","fields","relations"],exporter:new S(y,l)});const{syncDirectusCollections:t}=await p(f.schemaDir,"directus_config.js"),{syncCustomCollections:e}=await p(f.schemaDir,"config.js");if(b.addCollectionExporter(t,_),b.addCollectionExporter(e,_),a.SCHEMA_SYNC_CONFIG){const{syncCustomCollections:t}=await p(f.schemaDir,a.SCHEMA_SYNC_CONFIG);t?b.addCollectionExporter(t,_):l.warn('Additonal config specified but not exporting "syncCustomCollections"')}}return b},C=h((async(t=!0)=>{const e=await f.updateExportMeta();t&&e&&await E.lockForUpdates(e.hash,e.ts)&&await E.commitUpdates()}));function N(){"BOTH"!==a.SCHEMA_SYNC&&"EXPORT"!==a.SCHEMA_SYNC||k().then((t=>t.attachAllWatchers(e,C)))}"BOTH"===a.SCHEMA_SYNC||"IMPORT"===a.SCHEMA_SYNC?i("app.before",(async()=>{try{const t=await f.getExportMeta();if(!t)return l.info("Nothing exported yet it seems");if(!await E.lockForUpdates(t.hash,t.ts))return;l.info(`Updating schema and data with hash: ${t.hash}`);const e=await k();await e.loadAll(),await E.commitUpdates(),g=null}catch(t){l.error(t),l.info("Releasing lock..."),await E.releaseLock()}finally{await N()}})):N(),i("cli.before",(async({program:e})=>{const i=e.command("schema-sync");i.command("export-schema").description("Export only the schema file").action((async()=>{l.info("Exporting schema...");const t=new S(y,l);await t.export(),await C(),l.info("Done!"),process.exit(0)})),i.command("import-schema").description("Import only the schema file").action((async()=>{l.info("Importing schema...");const t=await f.getExportMeta();if(!t)return l.info("Nothing exported yet it seems");const e=new S(y,l);await e.load(),await E.forceCommitUpdates(t.hash,t.ts),l.info("Done!"),process.exit(0)})),i.command("install").description("Ensures the DB is ready for schema sync, and creates the schema-sync config folder").option("--force","Override schema-sync config folder").action((async({force:e})=>{l.info("Installing Schema sync..."),await E.ensureInstalled(),await async function(e,{logger:i}){const a=o.dirname(r(import.meta.url)),s=o.resolve(a,"../install"),n=process.cwd();e||await t.access(o.resolve(n,"schema-sync")).then((()=>{i.info("Config folder already exists, use --force to override"),process.exit(0)})).catch((()=>{i.info("Creating config folder...")})),await t.cp(s,n,{recursive:!0})}(e,{logger:l}),l.info("Done!"),process.exit(0)})),i.command("hash").description("Recalculate the hash for all the data files").action((async()=>{await C(!1),l.info("Done!"),process.exit(0)})),i.command("import").description("Import the schema and all available data from file to DB.").option("--merge","Only upsert data and not delete").action((async({merge:t})=>{try{l.info(`Importing everything from: ${f.dataDir}`);const e=await k();await e.loadAll(t),l.info("Done!"),process.exit(0)}catch(t){l.error(t),process.exit(1)}})),i.command("export").description("Export the schema and all data as configured from DB to file").action((async()=>{try{l.info(`Exporting everything to: ${f.dataDir}`);const t=await k();await t.exportAll(),await C(),l.info("Done!"),process.exit(0)}catch(t){l.error(t),process.exit(1)}}))}))};export{E as default};
